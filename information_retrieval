{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":83245,"databundleVersionId":9200365,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-16T11:46:06.791599Z","iopub.execute_input":"2024-08-16T11:46:06.791951Z","iopub.status.idle":"2024-08-16T11:46:07.808293Z","shell.execute_reply.started":"2024-08-16T11:46:06.791921Z","shell.execute_reply":"2024-08-16T11:46:07.807243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python -m spacy download uk_core_news_sm\n!pip install -q spacy\n!pip install -q stanza\n!pip install -q torch","metadata":{"execution":{"iopub.status.idle":"2024-08-16T11:47:19.349699Z","shell.execute_reply.started":"2024-08-16T11:46:17.564266Z","shell.execute_reply":"2024-08-16T11:47:19.348482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q rank_bm25 \n!pip install -q pymorphy2\n!pip install -q pymorphy2-dicts-uk","metadata":{"execution":{"iopub.status.busy":"2024-08-16T11:47:19.352018Z","iopub.execute_input":"2024-08-16T11:47:19.352656Z","iopub.status.idle":"2024-08-16T11:47:57.843223Z","shell.execute_reply.started":"2024-08-16T11:47:19.352617Z","shell.execute_reply":"2024-08-16T11:47:57.841916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Imports Libraries**","metadata":{}},{"cell_type":"code","source":"import json\n# import spacy\n# import string\nimport torch\nfrom tqdm.notebook import tqdm; tqdm.pandas();\nfrom spacy.lang.uk.stop_words import STOP_WORDS\nimport stanza\n\nfrom tqdm import tqdm\n\nfrom nltk.tokenize import word_tokenize\n\nfrom rank_bm25 import BM25Okapi\n\n\nimport numpy as np\n\nfrom pymorphy2 import MorphAnalyzer","metadata":{"execution":{"iopub.status.busy":"2024-08-16T11:47:57.844914Z","iopub.execute_input":"2024-08-16T11:47:57.845228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.is_available()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp = stanza.Pipeline('uk',use_gpu = True,processors='tokenize,ner')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# nlp_spacy = spacy.load('uk_core_news_sm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Imports**","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/epam-information-retrieval-july-2024/test.csv')\ndf_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/epam-information-retrieval-july-2024/reference.json') as f:\n    json_data = json.load(f)\ndf = pd.DataFrame.from_dict(json_data, orient='index', columns=['text'])\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Preproccessed Functions**","metadata":{}},{"cell_type":"code","source":"morph = MorphAnalyzer(lang='uk')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nimport re\n\n\ndef preprocess_text(text):\n    text = ' '.join(text)\n    return [word.lemma for sentence in nlp(text).sentences for word in sentence.words]\n\n\ndef preprocess(text):\n    text = text.replace('\\n', ' ').replace('\\r', ' ').strip()\n    \n    text = re.sub(r'\\{([^}]+)\\}', '', text)\n    text = re.sub(r'\\s\\d+', '', text)\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    text = re.sub(r'\\s-+[^\\s]*', '', text)\n    tokens = word_tokenize(text.lower())\n    pattern = re.compile(r\"[а-яА-ЯіїІєЄґҐa-zA-Z-0-9]+\")\n#     text = re.sub(r'\\d+', '', text)\n    return [word for word in tokens if pattern.fullmatch(word) and word not in STOP_WORDS and len(word) > 1]\n\n#     return [word for word in tokens if word.isalnum() and word not in STOP_WORDS]\n \ndef extract_entities(text):\n    return [ent.text for ent in nlp(text).ents]\n\ndef stem_ukrainian_word(word):\n    return morph.parse(word)[0].normal_form","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"STOP_WORDS = set(['примітки' ,'якою','якому','чимось','їхній','їх','посилання','ге','ла','аби','км','тож','інф','млрд','будь-якому','будь-який','тис','понад','грн','мільйона','тис','яке','який','яку','млн','кмгод','км2','осібкм2','див','який','таки','доволі','яким','якими','іншими','тобто']) | STOP_WORDS\n# STOP_WORDS","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Preprocessing**","metadata":{}},{"cell_type":"code","source":"df_test['processed'] = df_test.question.progress_apply(preprocess)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['processed'] = df.text.progress_apply(preprocess)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Stemming**","metadata":{}},{"cell_type":"code","source":"df['stem'] = df['processed'].progress_apply(lambda x: [stem_ukrainian_word(word) for word in x])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['stem'] = df_test['processed'].progress_apply(lambda x: [stem_ukrainian_word(word) for word in x])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Most Frequent and Rare words**","metadata":{}},{"cell_type":"code","source":"from collections import Counter\ncnt = Counter()\nfor text in df[\"stem\"].values:\n    for word in text:\n        cnt[word] += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cnt.most_common(100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FREQWORDS = set([w for (w, wc) in cnt.most_common(30)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_rare_words = 20\nRAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\nRAREWORDS","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['processed'] = df.processed.progress_apply(lambda x: [word for word in x if word not in (RAREWORDS | FREQWORDS)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['processed'] = df_test.processed.progress_apply(lambda x: [word for word in x if word not in (RAREWORDS | FREQWORDS)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('df.csv',escapechar='\\\\')\ndf_test.to_csv('df_test.csv',index = False,escapechar='\\\\' )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_different_stages_processing(indx):\n    question = df_test.iloc[idx].question \n    print(question)\n    processed = ' '.join(df_test.iloc[idx].processed)\n    print(preprocess(question))\n    print(processed)\n    print()\n    print(df_test.iloc[idx].stem)\n    print('-'*100)\n    print(preprocess_text(processed))\n    print(extract_entities(question))\n\nindx = 0\nshow_different_stages_processing(indx)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Load Preprocessed Data**","metadata":{}},{"cell_type":"code","source":"# df.to_csv('df.csv',escapechar='\\\\')\n# df_test.to_csv('df_test.csv',index = False,escapechar='\\\\')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = pd.read_csv('/kaggle/input/processed/df.csv',index_col = 'Unnamed: 0')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_test = pd.read_csv('/kaggle/input/processed/df_test.csv',index_col =False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Named Extraction Entities**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **BM25**","metadata":{}},{"cell_type":"code","source":"corpus = df['stem'].tolist()\nbm25 = BM25Okapi(corpus)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def search_bm25(query, top_n=100):\n#     query_tokens = preprocess(query)\n    \n    scores = bm25.get_scores(query)\n    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_n]\n    return top_indices\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **First Stage of Reranking**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Print The Results**","metadata":{}},{"cell_type":"code","source":"# find for each\ndef find_all_d(queries):\n    indices = []\n    for query in tqdm(queries):\n        top_n_indices = search_bm25(query,top_n = 3)\n        indices.append(df.iloc[top_n_indices].index)\n    return pd.DataFrame({'question': queries, 'doc_id':indices})\n\ndef print_bm25_by_index(indx,stem = False):\n    query = df_test.iloc[indx].question\n    print(query)\n#     query_token = [word for word in df_test.iloc[indx].stem if word not in STOP_WORDS]\n    \n    \n#     query_token = [token for token in query_token if token != 'який']\n    query_token = df_test.iloc[indx].stem\n#     print(preprocess(df_test.iloc[indx].question))\n    print(query_token)\n#     print(preprocess_text(df_test.iloc[indx].processed))\n    print(1,'-'*100)\n    \n    top_n_indices = search_bm25(query_token,10)\n\n    if stem:\n        top_articles = [' '.join(item) for item in df.iloc[top_n_indices].stem.values]\n    else:\n        top_articles = df.iloc[top_n_indices].text.values\n#     top_articles = df.iloc[top_n_indices].processed_article_text.values\n    for idx,i in enumerate(top_articles):\n        print(''.join(i),'\\n')\n#     print(top_articles.iloc[0])\n        print(idx+2,'-'*100)\n#     print(top_articles.iloc[1])\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# q = 177\nq = 238\nprint_bm25_by_index(q)\n# print_bm25_by_index(q,stem = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print_bm25_by_index(q,stem = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Submission Creation**","metadata":{}},{"cell_type":"code","source":"\ndef create_submission_dataframe(df_test, top_n=3):\n    submission_data = []\n    \n    for indx in tqdm(range(len(df_test))):\n        query = df_test.iloc[indx].stem\n        top_indices = search_bm25(query,3)\n        submission_data.append(df.iloc[top_indices].index)\n    \n    # Create DataFrame with the indices\n    submission_df = pd.DataFrame({'question':df_test['question'].tolist(),'doc_id':submission_data})\n    \n    return submission_df\n\n# Example usage:\nsubmission_preprocessed = create_submission_dataframe(df_test, top_n=3)\n# print(submission_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_preprocessed['doc_id'] = submission_preprocessed['doc_id'].progress_apply(lambda x: ' '.join(x))\nsubmission_preprocessed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_preprocessed.to_csv('submission.csv',index =False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}